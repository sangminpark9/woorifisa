# ML03
```python
from sklearn import datasets
import numpy as np
import pandas as pd

iris = datasets.load_iris()
iris
```

```python
X = iris.data[:, [2, 3]] # 독립변수
y = iris.target # 종속변수
```

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape
```
> random_state를 통해 잘 섞기
> 근데, test set에 패턴이 없다면?

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_train.shape, X_test.shape, y_train.shape, y_test.shape
```
> stratify -> 계층화 진행

# Perceptron 연습
```python
# 필요한 라이브러리 임포트
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Perceptron
from sklearn.metrics import accuracy_score

iris = load_iris()
X, y = iris.data, iris.target
# 1. 데이터를 필요한 방식으로 나눕니다
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42, stratify=y)

# 2. 모델을 만듭니다
model = Perceptron(random_state = 42)
# 3. 모델에 train 데이터를 학습시킵니다
model.fit(X_train, y_train)
# 4. 학습시킨 모델로 새로운 데이터를 예측합니다
y_pred = model.predict(X_test)
# 5. 모델이 예측한 결과를 실제값과 비교해서 정확도를 확인합니다
ac = accuracy_score(y_test, y_pred)
print(ac)
```
> ac가 train_test_split, test_size의 크기에 따라도 많이 변한다
> Perceptron()의 파라미터 중에 eta의 학습률 변동도 가능

# 활성화 함수

| **특성** | **선형 함수** | **시그모이드 함수** |
|:-:|:-:|:-:|
| 수식 | f(x) = ax + b | f(x) = 1 / (1 + e^(-x)) |
| 그래프 모양 | 직선 | S자 형태의 곡선 |
| 출력 범위 | (-∞, +∞) | (0, 1) |
| 미분 가능성 | 모든 점에서 미분 가능 | 모든 점에서 미분 가능 |
| 비선형성 | 없음 (선형) | 있음 (비선형) |
| 주요 용도 | 선형 회귀, 간단한 예측 모델 | 이진 분류, 로지스틱 회귀 |
| 기울기 소실 문제 | 발생하지 않음 | 입력값이 매우 크거나 작을 때 발생 가능 |
| 계산 복잡도 | 낮음 | 상대적으로 높음 (지수 함수 계산 필요) |

# 로지스틱 회귀
## 로지스틱 비용 함수의 가중치 학습
* sklearn API
  * sklearn.linear_model.LogisticRegression
* 생성된 회귀 모델에 대한 평가를 위해 LinearRegression 객체에는 두 개의 속성 값을 간직합니다.
* intercept_: 추정된 상수항
* coef_: 추정된 가중치 벡터


# 로지스틱 회귀 모델 설명 및 예측 과정

로지스틱 회귀 모델에서 `X_test` 값 `[0.55777524, 0.02224751]`를 넣었을 때의 결과를 로지스틱 회귀 수식으로 설명해보겠습니다.

## 1. 로지스틱 회귀 기본 수식

로지스틱 회귀의 예측값 $\hat{y}$은 아래의 수식을 통해 계산됩니다:

$$\hat{y} = w_0 + w_1x_1 + w_2x_2$$

이 수식에서:
* $w_0$는 상수항(bias, intercept)입니다.
* $w_1$과 $w_2$는 각각 피처 $x_1$, $x_2$에 대응하는 계수입니다.

하지만 이 수식은 단순 선형 회귀의 수식입니다. 로지스틱 회귀에서는 최종적으로 시그모이드 함수를 적용하여 0~1 사이의 확률 값을 얻습니다. 다중 클래스 분류의 경우 각 클래스에 대해 확률을 계산하여 그 중 가장 높은 확률을 가지는 클래스를 예측합니다.

## 2. 주어진 계수 값

이 모델은 3개의 클래스를 구분하는 모델이므로 각 클래스에 대해 하나의 방정식이 존재합니다. 주어진 계수들을 보면:

1. 클래스 0에 대한 방정식: 
   $$\hat{y_0} = -0.55733357 + (-2.41771318) \cdot x_1 + (-2.15202362) \cdot x_2$$

2. 클래스 1에 대한 방정식: 
   $$\hat{y_1} = 1.9034471 + 0.11601681 \cdot x_1 + (-0.36936809) \cdot x_2$$

3. 클래스 2에 대한 방정식: 
   $$\hat{y_2} = -1.34611353 + 2.30169637 \cdot x_1 + 2.5213917 \cdot x_2$$

## 3. `X_test = [0.55777524, 0.02224751]` 대입

이제 `X_test` 값을 각 방정식에 대입해보겠습니다.

1. 클래스 0에 대해:
   $$\hat{y_0} = -0.55733357 + (-2.41771318) \cdot 0.55777524 + (-2.15202362) \cdot 0.02224751$$
   
   계산 결과:
   $$\hat{y_0} \approx -0.55733357 + (-1.34864704) + (-0.04791624) \approx -1.95389685$$

2. 클래스 1에 대해:
   $$\hat{y_1} = 1.9034471 + 0.11601681 \cdot 0.55777524 + (-0.36936809) \cdot 0.02224751$$
   
   계산 결과:
   $$\hat{y_1} \approx 1.9034471 + 0.06470456 + (-0.00821607) \approx 1.95993559$$

3. 클래스 2에 대해:
   $$\hat{y_2} = -1.34611353 + 2.30169637 \cdot 0.55777524 + 2.5213917 \cdot 0.02224751$$
   
   계산 결과:
   $$\hat{y_2} \approx -1.34611353 + 1.28361702 + 0.05611619 \approx -0.00638032$$

## 4. 시그모이드 함수 적용

로지스틱 회귀에서는 이 선형 결합 값을 확률로 변환하기 위해 시그모이드 함수를 사용합니다:

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

계산한 각 클래스에 대한 값을 시그모이드 함수에 넣어 확률을 구합니다.

1. 클래스 0의 확률: 
   $$P(y=0) = \frac{1}{1 + e^{1.95389685}} \approx 0.12415$$

2. 클래스 1의 확률: 
   $$P(y=1) = \frac{1}{1 + e^{-1.95993559}} \approx 0.87648$$

3. 클래스 2의 확률: 
   $$P(y=2) = \frac{1}{1 + e^{0.00638032}} \approx 0.49840$$

## 5. 최종 예측

위에서 계산한 확률 중에서 가장 높은 값이 있는 클래스를 예측하게 됩니다. 클래스 1의 확률이 가장 높으므로, 이 모델은 `X_test` 값에 대해 클래스 1을 예측하게 됩니다.

## 요약

로지스틱 회귀 모델에서 `X_test = [0.55777524, 0.02224751]`을 입력했을 때, 클래스 1이 예측됩니다. 이는 클래스 1의 확률이 약 87.65%로 가장 높기 때문입니다.

# SVM
```python
# 모델 import
from sklearn.svm import SVC

# 모델을 인스턴스화
svm = SVC(,)

# 현재 결정경계와 가장 가까이 있는 점들을 기준으로 최대한 반반으로 나눌 수 있는 새 결정경계를 만듭니다
svm.fit(X_train_std, y_train)

svm.score(X_test_std, y_test) #
# gamma 값이라는 옵션을 통해서 각 데이터 포인트가 영향력을 행사하는 거리를 줄 수 있습니다.
# gamma는 작을수록 커지고, 클수록 하나의 데이터포인트가 주는 영향력이 작아집니다
plot_decision_regions(X=X_train_std, y=y_train, classifier=svm)
```

<img width="543" alt="image" src="https://github.com/user-attachments/assets/251e4a5c-95c2-4102-b48d-5bdf486c3dd5">

> SVM은 선형회귀이다. 근데, 곡선이 보이게 되는데, 차원을 다른 곳에서 바라봤을 때 직선이라는 것임.
> 아래 그림이 잘 나타내주고 있음.

<img width="693" alt="image" src="https://github.com/user-attachments/assets/46c6f7c3-2e09-4b21-bfc7-a29ee578ea2e">


# 결정트리

```python
from sklearn.tree import DecisionTreeClassifier

tree_model = DecisionTreeClassifier()
tree_model.fit(X_train, y_train)
tree_model.score(X_test, y_test)
```

```python
# w1x1 + w2x2 + b = yhat 

# x1이라는 특성 x2라는 특성의 관계가 분류에 영향을 미친다 


# 특성이 몇 개가 됐건 특성과 특성의 관계가 x
#                                 한 특성 안에서 불순도가 0이 되는 지점까지 
#                                 - 스케일링을 안 해도 해도 같은 결과가 나온다 
 # 특성중요도- x[0]은 6%, x[1]은 93.4%의 영향력을 가진 특성이다 
from sklearn.tree import DecisionTreeClassifier


tree_model = DecisionTreeClassifier()
tree_model.fit(X_train, y_train)
print(tree_model.score(X_test, y_test))


from sklearn import tree

tree.plot_tree(tree_model)
print(tree_model.feature_importances_)
```

