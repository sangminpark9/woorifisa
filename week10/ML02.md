# 1. ML02
## 머신러닝의 주요 작업 : 학습 알고리즘을 선택해서 데이터에 훈련시키는 것

* **나쁜 데이터**
  * 충분하지 않은 양의 데이터
  * 대표성 없는 데이터
  * 낮은 품질의 데이터 : 오류, 잡음, 이상치
  * 연관성이 적은 특성(변수)
* **나쁜 알고리즘**
  * 훈련 데이터 과대적합
  * 훈련 데이터 과소적합

# 2. 머신러닝에서 사용되는 주요 패키지
머신러닝 패키지
* 사이킷런(Scikit-Learn)

⠀배열/선형대수/통계 패키지
* NumPy
* SciPy

⠀데이터 핸들링
* Pandas

⠀시각화
* Matplotlib
* Seaborn
* Plotly

⠀딥러닝
* 텐서플로(Tensorflow)
* 케라스(Keras)
* 파이토치(Pytorch)

# 3. 뉴런 생성, 선형회귀 방법
1. 경사 강하법
```python
w = np.random.uniform(0, 1)  # yhat = wx + b 에서 초기의 w는 랜덤한 값을 부여하게 됩니다.

# 우리의 목표
x = 1
y = 0
eta = 0.1 # 학습율 (learning rate)을 임의로 부여하겠습니다.

for i in range(1000):
    output = w * x
    error = y - output # 예측한 값(output)과 실제값(y) 사이의 오차
    w = w + (x*error) * eta # 기존 학습결과의 eta 만큼을 이번 예측에 반영하도록

    print(f'학습횟수: {i}, 들어온 값(x): {x}, 예측결과(output): {output}, 오차(error): {error}, 가중치(w): {w}')
```

2. 정규 방정식
```python
import numpy as np

X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 5, 4, 5])

X_b = np.c_[np.ones((len(X), 1)), X]  # 편향을 위해 1을 추가
theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)

print(f'최적의 가중치: {theta[1]}, 편향: {theta[0]}')
```

3. 확률적 경사 강하법
```python
import numpy as np

X = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 5, 4, 5])

w = np.random.randn()
b = np.random.randn()
eta = 0.01

for epoch in range(1000):
    for i in range(len(X)):
        idx = np.random.randint(0, len(X))
        x_i = X[idx]
        y_i = y[idx]
        
        y_pred = w * x_i + b
        error = y_i - y_pred
        
        w += eta * error * x_i
        b += eta * error
        
    if epoch % 100 == 0:
        print(f'Epoch {epoch}, w: {w}, b: {b}')
```

4. 미니 배치 경사 강하법
```python
import numpy as np

X = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 5, 4, 5])

w = np.random.randn()
b = np.random.randn()
eta = 0.01
batch_size = 2

for epoch in range(1000):
    indices = np.random.permutation(len(X))
    for start_idx in range(0, len(X), batch_size):
        batch_idx = indices[start_idx:start_idx+batch_size]
        x_batch = X[batch_idx]
        y_batch = y[batch_idx]
        
        y_pred = w * x_batch + b
        error = y_batch - y_pred
        
        w += eta * np.mean(error * x_batch)
        b += eta * np.mean(error)
        
    if epoch % 100 == 0:
        print(f'Epoch {epoch}, w: {w}, b: {b}')
```

# 4. 파이썬으로 퍼셉트론 학습 알고리즘 구현
1. 객체생성
```python
import numpy as np

# w1x1 + w2x2 + b = yhat _ 모델이 스스로 변경하는 파라미터 -> 파라미터
# 개발자가 최초로 제공하는 파라미터 -> 하이퍼 파라미터

# 뉴런을 찍어내는 클래스
class Perceptron():
  # 기본적인 하이퍼파라미터들을 속성으로 가지고 있어야 할 것
  def _init_(self, eta=0.01, n_iter=50, random_state=1):
    self.eta = eta
    self.n_iter = n_iter
    self.random_state = random_state

  # X(훈련데이터) -> y(Label) 
  # X - 독립변수 (독립사건으로 인해서 나오게 된 변수 - 상수처럼 값을 집어넣을 뿐)
  # y - X의 결과로 나오게되는 종속변수
  # 학습을 시키기 위한 함수
  def fit(self, X, y):
    rgen = np.random.RandomState(self.random_state)
    # w1x1 + w2x2 + b 
    self.w_ = rgen.normal(loc=0.0, scale=0.01, size = 1+X.shape[1]) # 들어오는 X의 컬럼수(특성의 개수)만큼 가중치를 만들고
    self.errors_ = [] # 1개의 편향 - 학습할 때 마다 오차를 list로 저장해서 확인
    print(self.errors_)

    #실제 학습
    for _ in range(self.n_iter):
      errors = 0 

      for xi, target in zip(X,y):
        update = self.eta * (target - self.predit(xi))
        self.w_[1:] += update * xi # w1~wz 까지의 가중치에 오차의 학습률만큼 곱해서 반영
        self.w_[0] += update # bias에 오차를 학습률만큼 곱해서 반영
        errors += update

      self.errors_.append(errors)
    return self

    # predict - 계산된 결과를 출력하는 함수
    # 입력된 값이 0.1 이상이면 클래스 1, 그렇지 않으면 -1로 예측하는
  def predict(self,X):
    return np.where(self.net_input>0,1,-1)

    # net_input 학습이 완료된 모델에 새로 들어온 데이터를 계산하는 함수
    def net_input(self, X):
      # w1x1 + w2x2 + ... + b
      return np.dot(X, self.w_[1:]) + self.w_[0]

ppn = Perceptron()
```

```python
import os
import pandas as pd


s = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
print('URL:', s)

df = pd.read_csv(s, header=None, encoding='utf-8')
df.tail()
```
|  | **0** | **1** | **2** | **3** | **4** |
|--:|--:|--:|--:|--:|--:|
| **145** | 6.7 | 3.0 | 5.2 | 2.3 | Iris-virginica |
| **146** | 6.3 | 2.5 | 5.0 | 1.9 | Iris-virginica |
| **147** | 6.5 | 3.0 | 5.2 | 2.0 | Iris-virginica |
| **148** | 6.2 | 3.4 | 5.4 | 2.3 | Iris-virginica |
| **149** | 5.9 | 3.0 | 5.1 | 1.8 | Iris-virginica |

```python
df.describe(include='all')
```
> [{"0":"150.0","1":"150.0","2":"150.0","3":"150.0","4":"150","index":"count"},{"0":"NaN","1":"NaN","2":"NaN","3":"NaN","4":"3","index":"unique"},{"0":"NaN","1":"NaN","2":"NaN","3":"NaN","4":"Iris-setosa","index":"top"},{"0":"NaN","1":"NaN","2":"NaN","3":"NaN","4":"50","index":"freq"},{"0":"5.843333333333334","1":"3.0540000000000003","2":"3.758666666666666","3":"1.1986666666666668","4":"NaN","index":"mean"},{"0":"0.828066127977863","1":"0.4335943113621737","2":"1.7644204199522626","3":"0.7631607417008411","4":"NaN","index":"std"},{"0":"4.3","1":"2.0","2":"1.0","3":"0.1","4":"NaN","index":"min"},{"0":"5.1","1":"2.8","2":"1.6","3":"0.3","4":"NaN","index":"25%"},{"0":"5.8","1":"3.0","2":"4.35","3":"1.3","4":"NaN","index":"50%"},{"0":"6.4","1":"3.3","2":"5.1","3":"1.8","4":"NaN","index":"75%"},{"0":"7.9","1":"4.4","2":"6.9","3":"2.5","4":"NaN","index":"max"}]

```python
# X (특성, 독립변수) -> y (Class/Target, 종속변수)
X = np.array(df.iloc[:100, :4])
y = np.where(df.iloc[:100, 4] == 'Iris-setosa', -1, 1) # -1 : setosa, 1: versicolor
```

```python
# 학습
ppn.fit(X, y)
ppn.w_
ppn.predict(np.array([5.2, 2.7, 1.4, 0.2]))
```

